# Aurora

## Paper

[Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases](https://web.stanford.edu/class/cs245/readings/aurora.pdf)

## Short description

Amazon Aurora is a relational database service for OLTP workloads offered as part of Amazon Web Services (AWS).

## Unnamed

In Aurora, we have chosen a design point of tolerating 
(a) losing an entire AZ and one additional node (AZ+1) without losing data, and 
(b) losing an entire AZ without impacting the ability to write data. 
We achieve this by replicating each data item 6 ways across 3 AZs with 2 copies of each item in each AZ. 
We use a quorum model with 6 votes (V = 6), a write quorum of 4/6 (Vw = 4), and a read quorum of 3/6 (Vr = 3). 
With such a model, we can 
(a) lose a single AZ and one additional node (a failure of 3 nodes) without losing read availability, and 
(b) lose any two nodes, including a single AZ failure and maintain write availability. 
Ensuring read quorum enables us to rebuild write quorum by adding additional replica copies.


Since our system has a high tolerance to failures, we can leverage this for maintenance operations that cause segment unavailability. 
For example, heat management is straightforward. 
We can mark one of the segments on a hot disk or node as bad, and the quorum will be quickly repaired by migration to some other colder node in the fleet. 
OS and security patching is a brief unavailability event for that storage node as it is being patched. 
Even software upgrades to our storage fleet are managed this way. 
We execute them one AZ at a time and ensure no more than one member of a PG is being patched simultaneously. 
This allows us to use agile methodologies and rapid deployments in our storage service.

In Aurora, the only writes that cross the network are redo log records. 
No pages are ever written from the database tier, not for background writes, not for checkpointing, and not for cache eviction. 
Instead, the log applicator is pushed to the storage tier where it can be used to generate database pages in background or on demand. 
Of course, generating each page from the complete chain of its modifications from the beginning of time is prohibitively expensive. 
We therefore continually materialize database pages in the background to avoid regenerating them from scratch on demand every time. 
Note that background materialization is entirely optional from the perspective of correctness: 
as far as the engine is concerned, the log is the database, and any pages that the storage system materializes are simply a cache of log applications.

In Aurora, background processing has negative correlation with foreground processing. 
This is unlike a traditional database, where background writes of pages and checkpointing have positive correlation with the foreground load on the system. 
If we build up a backlog on the system, we will throttle foreground activity to prevent a long queue buildup. 
Since segments are placed with high entropy across the various storage nodes in our system, 
throttling at one storage node is readily handled by our 4/6 quorum writes, appearing as a slow node.

In practice, each log record has an associated Log Sequence Number (LSN) that is a monotonically increasing value generated by the database.
The storage service determines the highest LSN for which it can guarantee availability of all prior log records (this is known as the VCL or Volume Complete LSN). During storage recovery, every log record with an LSN larger than the VCL must be truncated. The database can, however, further constrain a subset of points that are allowable for truncation by tagging log records and identifying them as CPLs or Consistency Point LSNs. We therefore define VDL or the Volume Durable LSN as the highest CPL that is smaller than or equal to VCL and truncate all log records with LSN greater than the VDL. For example, even if we have the complete data up to LSN 1007, the database may have declared that only 900, 1000, and 1100 are CPLs, in which case, we must truncate at 1000. We are complete to 1007, but only durable to 1000.
Completeness and durability are therefore different and a CPL can be thought of as delineating some limited form of storage system transaction that must be accepted in order. If the client has no use for such distinctions, it can simply mark every log record as a CPL. In practice, the database and storage interact as follows:
1. Each database-level transaction is broken up into multiple mini-transactions (MTRs) that are ordered and must be performed atomically.
2. Each mini-transaction is composed of multiple contiguous log records (as many as needed).
3. The final log record in a mini-transaction is a CPL.
On recovery, the database talks to the storage service to establish the durable point of each PG and uses that to establish the VDL and then issue commands to truncate the log records above VDL.


